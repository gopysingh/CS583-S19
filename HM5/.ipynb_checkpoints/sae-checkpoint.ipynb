{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HM5: Build a Supervised Autoencoder.\n",
    "\n",
    "### Name: [Gurpreet Singh]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "PCA and the standard autoencoder are unsupervised dimensionality reduction methods, and their learned features are not discriminative. If you build a classifier upon the low-dimenional features extracted by PCA and autoencoder, you will find the classification accuracy very poor.\n",
    "\n",
    "Linear discriminant analysis (LDA) is a traditionally supervised dimensionality reduction method for learning low-dimensional features which are highly discriminative. Likewise, can we extend autoencoder to supervised leanring?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You are required to build and train a supervised autoencoder look like the following.** You are required to add other layers properly to alleviate overfitting.\n",
    "\n",
    "\n",
    "![Network Structure](https://github.com/wangshusen/CS583A-2019Spring/blob/master/homework/HM5/supervised_ae.png?raw=true \"NetworkStructure\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. You will do the following:\n",
    "\n",
    "1. Read and run my code to train a standard dense autoencoder.\n",
    "\n",
    "2. Build and train a supervised autoencoder, visual the low-dim features and the reconstructions, and evaluate whether the learned low-dim features are discriminative.\n",
    "    \n",
    "3. Convert the .IPYNB file to .HTML file.\n",
    "\n",
    "    * The HTML file must contain the code and the output after execution.\n",
    "    \n",
    "    \n",
    "    \n",
    "4. Upload this .HTML file to your Github repo.\n",
    "\n",
    "4. Submit the link to this .HTML file to Canvas.\n",
    "\n",
    "    * Example: https://github.com/wangshusen/CS583A-2019Spring/blob/master/homework/HM5/sae.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_train: (60000, 784)\n",
      "Shape of x_test: (10000, 784)\n",
      "Shape of y_train: (60000,)\n",
      "Shape of y_test: (10000,)\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.reshape(60000, 28*28).astype('float32') / 255.\n",
    "x_test = x_test.reshape(10000, 28*28).astype('float32') / 255.\n",
    "\n",
    "print('Shape of x_train: ' + str(x_train.shape)) \n",
    "print('Shape of x_test: ' + str(x_test.shape))\n",
    "print('Shape of y_train: ' + str(y_train.shape))\n",
    "print('Shape of y_test: ' + str(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. One-hot encode the labels\n",
    "\n",
    "In the input, a label is a scalar in $\\{0, 1, \\cdots , 9\\}$. One-hot encode transform such a scalar to a $10$-dim vector. E.g., a scalar ```y_train[j]=3``` is transformed to the vector ```y_train_vec[j]=[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]```.\n",
    "\n",
    "1. Define a function ```to_one_hot``` that transforms an $n\\times 1$ array to a $n\\times 10$ matrix.\n",
    "\n",
    "2. Apply the function to ```y_train``` and ```y_test```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y_train_vec: (60000, 10)\n",
      "Shape of y_test_vec: (10000, 10)\n",
      "5\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "\n",
    "def to_one_hot(y, num_class=10):\n",
    "    results = numpy.zeros((len(y), num_class))\n",
    "    for i, label in enumerate(y):\n",
    "        results[i, label] = 1.\n",
    "    return results\n",
    "\n",
    "y_train_vec = to_one_hot(y_train)\n",
    "y_test_vec = to_one_hot(y_test)\n",
    "\n",
    "print('Shape of y_train_vec: ' + str(y_train_vec.shape))\n",
    "print('Shape of y_test_vec: ' + str(y_test_vec.shape))\n",
    "\n",
    "print(y_train[0])\n",
    "print(y_train_vec[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Randomly partition the training set to training and validation sets\n",
    "\n",
    "Randomly partition the 60K training samples to 2 sets:\n",
    "* a training set containing 10K samples\n",
    "* a validation set containing 50K samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_tr: (10000, 784)\n",
      "Shape of y_tr: (10000, 10)\n",
      "Shape of x_val: (10000, 784)\n",
      "Shape of y_val: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "rand_indices = numpy.random.permutation(60000)\n",
    "train_indices = rand_indices[0:10000]\n",
    "valid_indices = rand_indices[10000:20000]\n",
    "\n",
    "x_val = x_train[valid_indices, :]\n",
    "y_val = y_train_vec[valid_indices, :]\n",
    "\n",
    "x_tr = x_train[train_indices, :]\n",
    "y_tr = y_train_vec[train_indices, :]\n",
    "\n",
    "print('Shape of x_tr: ' + str(x_tr.shape))\n",
    "print('Shape of y_tr: ' + str(y_tr.shape))\n",
    "print('Shape of x_val: ' + str(x_val.shape))\n",
    "print('Shape of y_val: ' + str(y_val.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build an unsupervised  autoencoder and tune its hyper-parameters\n",
    "\n",
    "1. Build a dense autoencoder model\n",
    "2. Use the validation data to tune the hyper-parameters (e.g., network structure, and optimization algorithm)\n",
    "    * Do NOT use test data for hyper-parameter tuning!!!\n",
    "    \n",
    "3. Try to achieve a validation loss as low as possible.\n",
    "4. Evaluate the model on the test set.\n",
    "5. Visualize the low-dim features and reconstructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_img (InputLayer)       (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "encode1 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "encode2 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "encode3 (Dense)              (None, 8)                 264       \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 8)                 32        \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "bottleneck (Dense)           (None, 2)                 18        \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 2)                 8         \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "decode1 (Dense)              (None, 8)                 24        \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 8)                 32        \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "decode2 (Dense)              (None, 32)                288       \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "decode3 (Dense)              (None, 128)               4224      \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "decode4 (Dense)              (None, 784)               101136    \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 784)               3136      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 784)               0         \n",
      "=================================================================\n",
      "Total params: 215,050\n",
      "Trainable params: 212,806\n",
      "Non-trainable params: 2,244\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Input, Dropout, Activation, BatchNormalization\n",
    "from keras import models\n",
    "\n",
    "input_img = Input(shape=(784,), name='input_img')\n",
    "\n",
    "encode1 = Dense(128, activation='relu', name='encode1')(input_img)\n",
    "encode1 = BatchNormalization()(encode1)\n",
    "encode1 = Activation(\"elu\")(encode1)\n",
    "\n",
    "encode2 = Dense(32, activation='relu', name='encode2')(encode1)\n",
    "encode2 = BatchNormalization()(encode2)\n",
    "encode2 = Activation(\"elu\")(encode2)\n",
    "\n",
    "encode3 = Dense(8, activation='relu', name='encode3')(encode2)\n",
    "encode3 = BatchNormalization()(encode3)\n",
    "encode3 = Activation(\"elu\")(encode3)\n",
    "\n",
    "bottleneck = Dense(2, activation='relu', name='bottleneck')(encode3)\n",
    "bottleneck = BatchNormalization()(bottleneck)\n",
    "bottleneck = Activation(\"elu\")(bottleneck)\n",
    "\n",
    "decode1 = Dense(8, activation='relu', name='decode1')(bottleneck)\n",
    "decode1 = BatchNormalization()(decode1)\n",
    "decode1 = Activation(\"elu\")(decode1)\n",
    "\n",
    "decode2 = Dense(32, activation='relu', name='decode2')(decode1)\n",
    "decode2 = BatchNormalization()(decode2)\n",
    "decode2 = Activation(\"elu\")(decode2)\n",
    "\n",
    "decode3 = Dense(128, activation='relu', name='decode3')(decode2)\n",
    "decode3 = BatchNormalization()(decode3)\n",
    "decode3 = Activation(\"elu\")(decode3)\n",
    "\n",
    "decode4 = Dense(784, activation='relu', name='decode4')(decode3)\n",
    "decode4 = BatchNormalization()(decode4)\n",
    "decode4 = Activation(\"elu\")(decode4)\n",
    "\n",
    "ae = models.Model(input_img, decode4)\n",
    "\n",
    "ae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the network structure to a PDF file\n",
    "\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot, plot_model\n",
    "\n",
    "SVG(model_to_dot(ae, show_shapes=False).create(prog='dot', format='svg'))\n",
    "\n",
    "plot_model(\n",
    "    model=ae, show_shapes=False,\n",
    "    to_file='unsupervised_ae.pdf'\n",
    ")\n",
    "\n",
    "# you can find the file \"unsupervised_ae.pdf\" in the current directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Train the model and tune the hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "learning_rate = 1E-3\n",
    "\n",
    "ae.compile(loss='binary_crossentropy',\n",
    "           metrics=['acc'],\n",
    "           optimizer=optimizers.RMSprop(lr=learning_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "10000/10000 [==============================] - 3s 331us/step - loss: 0.7113 - acc: 0.7490 - val_loss: 0.8449 - val_acc: 0.7112\n",
      "Epoch 2/100\n",
      "10000/10000 [==============================] - 2s 170us/step - loss: 0.6284 - acc: 0.7583 - val_loss: 0.6806 - val_acc: 0.7228\n",
      "Epoch 3/100\n",
      "10000/10000 [==============================] - 2s 160us/step - loss: 0.5492 - acc: 0.7672 - val_loss: 0.5402 - val_acc: 0.7338\n",
      "Epoch 4/100\n",
      "10000/10000 [==============================] - 2s 171us/step - loss: 0.4833 - acc: 0.7726 - val_loss: 0.5046 - val_acc: 0.7339\n",
      "Epoch 5/100\n",
      "10000/10000 [==============================] - 2s 174us/step - loss: 0.4263 - acc: 0.7765 - val_loss: 0.4287 - val_acc: 0.7487\n",
      "Epoch 6/100\n",
      "10000/10000 [==============================] - 2s 165us/step - loss: 0.3991 - acc: 0.7775 - val_loss: 0.4279 - val_acc: 0.7585\n",
      "Epoch 7/100\n",
      "10000/10000 [==============================] - 2s 168us/step - loss: 0.3756 - acc: 0.7789 - val_loss: 0.3778 - val_acc: 0.7656\n",
      "Epoch 8/100\n",
      "10000/10000 [==============================] - 2s 168us/step - loss: 0.3564 - acc: 0.7800 - val_loss: 0.3853 - val_acc: 0.7736\n",
      "Epoch 9/100\n",
      "10000/10000 [==============================] - 2s 173us/step - loss: 0.3486 - acc: 0.7813 - val_loss: 0.3770 - val_acc: 0.7783\n",
      "Epoch 10/100\n",
      "10000/10000 [==============================] - 2s 163us/step - loss: 0.3361 - acc: 0.7825 - val_loss: 0.3722 - val_acc: 0.7782\n",
      "Epoch 11/100\n",
      "10000/10000 [==============================] - 2s 172us/step - loss: 0.3257 - acc: 0.7839 - val_loss: 0.3349 - val_acc: 0.7824\n",
      "Epoch 12/100\n",
      "10000/10000 [==============================] - 2s 177us/step - loss: 0.3164 - acc: 0.7853 - val_loss: 0.3621 - val_acc: 0.7816\n",
      "Epoch 13/100\n",
      "10000/10000 [==============================] - 2s 170us/step - loss: 0.3118 - acc: 0.7861 - val_loss: 0.3546 - val_acc: 0.7805\n",
      "Epoch 14/100\n",
      "10000/10000 [==============================] - 2s 168us/step - loss: 0.3038 - acc: 0.7863 - val_loss: 0.3020 - val_acc: 0.7852\n",
      "Epoch 15/100\n",
      "10000/10000 [==============================] - 2s 169us/step - loss: 0.2999 - acc: 0.7868 - val_loss: 0.3246 - val_acc: 0.7847\n",
      "Epoch 16/100\n",
      "10000/10000 [==============================] - 2s 169us/step - loss: 0.2958 - acc: 0.7870 - val_loss: 0.3347 - val_acc: 0.7863\n",
      "Epoch 17/100\n",
      "10000/10000 [==============================] - 2s 182us/step - loss: 0.2930 - acc: 0.7872 - val_loss: 0.3212 - val_acc: 0.7870\n",
      "Epoch 18/100\n",
      "10000/10000 [==============================] - 2s 181us/step - loss: 0.2915 - acc: 0.7874 - val_loss: 0.3412 - val_acc: 0.7854\n",
      "Epoch 19/100\n",
      "10000/10000 [==============================] - 2s 179us/step - loss: 0.2888 - acc: 0.7879 - val_loss: 0.2936 - val_acc: 0.7885\n",
      "Epoch 20/100\n",
      "10000/10000 [==============================] - 2s 174us/step - loss: 0.2868 - acc: 0.7880 - val_loss: 0.3123 - val_acc: 0.7877\n",
      "Epoch 21/100\n",
      "10000/10000 [==============================] - 2s 177us/step - loss: 0.2857 - acc: 0.7884 - val_loss: 0.2814 - val_acc: 0.7893\n",
      "Epoch 22/100\n",
      "10000/10000 [==============================] - 2s 180us/step - loss: 0.2839 - acc: 0.7888 - val_loss: 0.2839 - val_acc: 0.7890\n",
      "Epoch 23/100\n",
      "10000/10000 [==============================] - 2s 166us/step - loss: 0.2829 - acc: 0.7890 - val_loss: 0.2814 - val_acc: 0.7892\n",
      "Epoch 24/100\n",
      "10000/10000 [==============================] - 2s 171us/step - loss: 0.2823 - acc: 0.7894 - val_loss: 0.2811 - val_acc: 0.7894\n",
      "Epoch 25/100\n",
      "10000/10000 [==============================] - 2s 186us/step - loss: 0.2820 - acc: 0.7898 - val_loss: 0.2829 - val_acc: 0.7900\n",
      "Epoch 26/100\n",
      "10000/10000 [==============================] - 2s 180us/step - loss: 0.2821 - acc: 0.7900 - val_loss: 0.2818 - val_acc: 0.7905\n",
      "Epoch 27/100\n",
      "10000/10000 [==============================] - 2s 171us/step - loss: 0.2815 - acc: 0.7901 - val_loss: 0.2782 - val_acc: 0.7915\n",
      "Epoch 28/100\n",
      "10000/10000 [==============================] - 2s 173us/step - loss: 0.2802 - acc: 0.7901 - val_loss: 0.2781 - val_acc: 0.7901\n",
      "Epoch 29/100\n",
      "10000/10000 [==============================] - 2s 161us/step - loss: 0.2800 - acc: 0.7901 - val_loss: 0.2780 - val_acc: 0.7901\n",
      "Epoch 30/100\n",
      "10000/10000 [==============================] - 2s 160us/step - loss: 0.2788 - acc: 0.7900 - val_loss: 0.3068 - val_acc: 0.7888\n",
      "Epoch 31/100\n",
      "10000/10000 [==============================] - 2s 176us/step - loss: 0.2791 - acc: 0.7901 - val_loss: 0.2748 - val_acc: 0.7905\n",
      "Epoch 32/100\n",
      "10000/10000 [==============================] - 2s 178us/step - loss: 0.2788 - acc: 0.7899 - val_loss: 0.2731 - val_acc: 0.7908\n",
      "Epoch 33/100\n",
      "10000/10000 [==============================] - 2s 182us/step - loss: 0.2776 - acc: 0.7902 - val_loss: 0.2749 - val_acc: 0.7901\n",
      "Epoch 34/100\n",
      "10000/10000 [==============================] - 2s 172us/step - loss: 0.2780 - acc: 0.7898 - val_loss: 0.2722 - val_acc: 0.7901\n",
      "Epoch 35/100\n",
      "10000/10000 [==============================] - 2s 154us/step - loss: 0.2772 - acc: 0.7899 - val_loss: 0.2713 - val_acc: 0.7901\n",
      "Epoch 36/100\n",
      "10000/10000 [==============================] - 2s 160us/step - loss: 0.2786 - acc: 0.7899 - val_loss: 0.2759 - val_acc: 0.7898\n",
      "Epoch 37/100\n",
      "10000/10000 [==============================] - 1s 149us/step - loss: 0.2774 - acc: 0.7901 - val_loss: 0.2729 - val_acc: 0.7915\n",
      "Epoch 38/100\n",
      "10000/10000 [==============================] - 2s 154us/step - loss: 0.2770 - acc: 0.7901 - val_loss: 0.2720 - val_acc: 0.7917\n",
      "Epoch 39/100\n",
      "10000/10000 [==============================] - 2s 150us/step - loss: 0.2773 - acc: 0.7900 - val_loss: 0.2708 - val_acc: 0.7896\n",
      "Epoch 40/100\n",
      "10000/10000 [==============================] - 2s 152us/step - loss: 0.2763 - acc: 0.7901 - val_loss: 0.2717 - val_acc: 0.7907\n",
      "Epoch 41/100\n",
      "10000/10000 [==============================] - 1s 145us/step - loss: 0.2764 - acc: 0.7898 - val_loss: 0.2795 - val_acc: 0.7916\n",
      "Epoch 42/100\n",
      "10000/10000 [==============================] - 2s 154us/step - loss: 0.2764 - acc: 0.7900 - val_loss: 0.2701 - val_acc: 0.7920\n",
      "Epoch 43/100\n",
      "10000/10000 [==============================] - 1s 148us/step - loss: 0.2767 - acc: 0.7898 - val_loss: 0.2787 - val_acc: 0.7905\n",
      "Epoch 44/100\n",
      "10000/10000 [==============================] - 2s 150us/step - loss: 0.2764 - acc: 0.7900 - val_loss: 0.2720 - val_acc: 0.7903\n",
      "Epoch 45/100\n",
      "10000/10000 [==============================] - 1s 142us/step - loss: 0.2746 - acc: 0.7902 - val_loss: 0.2732 - val_acc: 0.7907\n",
      "Epoch 46/100\n",
      "10000/10000 [==============================] - 1s 148us/step - loss: 0.2753 - acc: 0.7903 - val_loss: 0.2769 - val_acc: 0.7909\n",
      "Epoch 47/100\n",
      "10000/10000 [==============================] - 1s 149us/step - loss: 0.2759 - acc: 0.7902 - val_loss: 0.2696 - val_acc: 0.7906\n",
      "Epoch 48/100\n",
      "10000/10000 [==============================] - 1s 147us/step - loss: 0.2753 - acc: 0.7903 - val_loss: 0.2698 - val_acc: 0.7895\n",
      "Epoch 49/100\n",
      "10000/10000 [==============================] - 1s 149us/step - loss: 0.2755 - acc: 0.7901 - val_loss: 0.2690 - val_acc: 0.7899\n",
      "Epoch 50/100\n",
      "10000/10000 [==============================] - 1s 150us/step - loss: 0.2743 - acc: 0.7902 - val_loss: 0.2688 - val_acc: 0.7910\n",
      "Epoch 51/100\n",
      "10000/10000 [==============================] - 2s 150us/step - loss: 0.2742 - acc: 0.7904 - val_loss: 0.2680 - val_acc: 0.7912\n",
      "Epoch 52/100\n",
      "10000/10000 [==============================] - 2s 156us/step - loss: 0.2749 - acc: 0.7907 - val_loss: 0.2719 - val_acc: 0.7905\n",
      "Epoch 53/100\n",
      "10000/10000 [==============================] - 2s 162us/step - loss: 0.2739 - acc: 0.7904 - val_loss: 0.2699 - val_acc: 0.7904\n",
      "Epoch 54/100\n",
      "10000/10000 [==============================] - 2s 178us/step - loss: 0.2726 - acc: 0.7908 - val_loss: 0.2692 - val_acc: 0.7910\n",
      "Epoch 55/100\n",
      "10000/10000 [==============================] - 2s 166us/step - loss: 0.2733 - acc: 0.7908 - val_loss: 0.2697 - val_acc: 0.7908\n",
      "Epoch 56/100\n",
      "10000/10000 [==============================] - 2s 162us/step - loss: 0.2737 - acc: 0.7908 - val_loss: 0.2698 - val_acc: 0.7914\n",
      "Epoch 57/100\n",
      "10000/10000 [==============================] - 2s 163us/step - loss: 0.2735 - acc: 0.7908 - val_loss: 0.2676 - val_acc: 0.7911\n",
      "Epoch 58/100\n",
      "10000/10000 [==============================] - 2s 168us/step - loss: 0.2745 - acc: 0.7906 - val_loss: 0.2672 - val_acc: 0.7917\n",
      "Epoch 59/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 2s 177us/step - loss: 0.2728 - acc: 0.7908 - val_loss: 0.2672 - val_acc: 0.7922\n",
      "Epoch 60/100\n",
      "10000/10000 [==============================] - 2s 165us/step - loss: 0.2740 - acc: 0.7908 - val_loss: 0.2661 - val_acc: 0.7914\n",
      "Epoch 61/100\n",
      "10000/10000 [==============================] - 2s 159us/step - loss: 0.2734 - acc: 0.7910 - val_loss: 0.2658 - val_acc: 0.7917\n",
      "Epoch 62/100\n",
      "10000/10000 [==============================] - 2s 160us/step - loss: 0.2731 - acc: 0.7907 - val_loss: 0.2698 - val_acc: 0.7910\n",
      "Epoch 63/100\n",
      "10000/10000 [==============================] - 2s 171us/step - loss: 0.2726 - acc: 0.7910 - val_loss: 0.2691 - val_acc: 0.7918\n",
      "Epoch 64/100\n",
      "10000/10000 [==============================] - 2s 175us/step - loss: 0.2747 - acc: 0.7907 - val_loss: 0.2687 - val_acc: 0.7917\n",
      "Epoch 65/100\n",
      "10000/10000 [==============================] - 2s 159us/step - loss: 0.2740 - acc: 0.7911 - val_loss: 0.2688 - val_acc: 0.7922\n",
      "Epoch 66/100\n",
      "10000/10000 [==============================] - 1s 147us/step - loss: 0.2728 - acc: 0.7910 - val_loss: 0.2684 - val_acc: 0.7919\n",
      "Epoch 67/100\n",
      "10000/10000 [==============================] - 1s 144us/step - loss: 0.2725 - acc: 0.7912 - val_loss: 0.2728 - val_acc: 0.7929\n",
      "Epoch 68/100\n",
      "10000/10000 [==============================] - 1s 145us/step - loss: 0.2730 - acc: 0.7911 - val_loss: 0.2669 - val_acc: 0.7919\n",
      "Epoch 69/100\n",
      "10000/10000 [==============================] - 1s 143us/step - loss: 0.2731 - acc: 0.7913 - val_loss: 0.2688 - val_acc: 0.7911\n",
      "Epoch 70/100\n",
      "10000/10000 [==============================] - 1s 144us/step - loss: 0.2740 - acc: 0.7910 - val_loss: 0.2686 - val_acc: 0.7915\n",
      "Epoch 71/100\n",
      "10000/10000 [==============================] - 1s 146us/step - loss: 0.2724 - acc: 0.7912 - val_loss: 0.2663 - val_acc: 0.7923\n",
      "Epoch 72/100\n",
      "10000/10000 [==============================] - 2s 188us/step - loss: 0.2720 - acc: 0.7910 - val_loss: 0.2683 - val_acc: 0.7906\n",
      "Epoch 73/100\n",
      "10000/10000 [==============================] - 2s 181us/step - loss: 0.2728 - acc: 0.7910 - val_loss: 0.2691 - val_acc: 0.7919\n",
      "Epoch 74/100\n",
      "10000/10000 [==============================] - 2s 221us/step - loss: 0.2721 - acc: 0.7913 - val_loss: 0.2694 - val_acc: 0.7923\n",
      "Epoch 75/100\n",
      "10000/10000 [==============================] - 2s 192us/step - loss: 0.2726 - acc: 0.7912 - val_loss: 0.2670 - val_acc: 0.7914\n",
      "Epoch 76/100\n",
      "10000/10000 [==============================] - 2s 162us/step - loss: 0.2719 - acc: 0.7915 - val_loss: 0.2668 - val_acc: 0.7917\n",
      "Epoch 77/100\n",
      "10000/10000 [==============================] - 2s 155us/step - loss: 0.2724 - acc: 0.7913 - val_loss: 0.2654 - val_acc: 0.7923\n",
      "Epoch 78/100\n",
      "10000/10000 [==============================] - 2s 157us/step - loss: 0.2729 - acc: 0.7913 - val_loss: 0.2666 - val_acc: 0.7918\n",
      "Epoch 79/100\n",
      "10000/10000 [==============================] - 1s 149us/step - loss: 0.2730 - acc: 0.7911 - val_loss: 0.2654 - val_acc: 0.7918\n",
      "Epoch 80/100\n",
      "10000/10000 [==============================] - 2s 152us/step - loss: 0.2724 - acc: 0.7912 - val_loss: 0.2673 - val_acc: 0.7916\n",
      "Epoch 81/100\n",
      "10000/10000 [==============================] - 2s 151us/step - loss: 0.2718 - acc: 0.7917 - val_loss: 0.2658 - val_acc: 0.7925\n",
      "Epoch 82/100\n",
      "10000/10000 [==============================] - 1s 150us/step - loss: 0.2710 - acc: 0.7917 - val_loss: 0.2654 - val_acc: 0.7923\n",
      "Epoch 83/100\n",
      "10000/10000 [==============================] - 2s 151us/step - loss: 0.2710 - acc: 0.7918 - val_loss: 0.2637 - val_acc: 0.7923\n",
      "Epoch 84/100\n",
      "10000/10000 [==============================] - 2s 152us/step - loss: 0.2703 - acc: 0.7915 - val_loss: 0.2651 - val_acc: 0.7926\n",
      "Epoch 85/100\n",
      "10000/10000 [==============================] - 2s 196us/step - loss: 0.2703 - acc: 0.7918 - val_loss: 0.2665 - val_acc: 0.7931\n",
      "Epoch 86/100\n",
      "10000/10000 [==============================] - 2s 177us/step - loss: 0.2716 - acc: 0.7917 - val_loss: 0.2661 - val_acc: 0.7922\n",
      "Epoch 87/100\n",
      "10000/10000 [==============================] - 2s 171us/step - loss: 0.2705 - acc: 0.7917 - val_loss: 0.2653 - val_acc: 0.7927\n",
      "Epoch 88/100\n",
      "10000/10000 [==============================] - 2s 164us/step - loss: 0.2708 - acc: 0.7917 - val_loss: 0.2671 - val_acc: 0.7919\n",
      "Epoch 89/100\n",
      "10000/10000 [==============================] - 2s 160us/step - loss: 0.2710 - acc: 0.7916 - val_loss: 0.2681 - val_acc: 0.7927\n",
      "Epoch 90/100\n",
      "10000/10000 [==============================] - 2s 163us/step - loss: 0.2707 - acc: 0.7918 - val_loss: 0.2640 - val_acc: 0.7924\n",
      "Epoch 91/100\n",
      "10000/10000 [==============================] - 2s 161us/step - loss: 0.2703 - acc: 0.7919 - val_loss: 0.2645 - val_acc: 0.7928\n",
      "Epoch 92/100\n",
      "10000/10000 [==============================] - 2s 161us/step - loss: 0.2717 - acc: 0.7917 - val_loss: 0.2639 - val_acc: 0.7923\n",
      "Epoch 93/100\n",
      "10000/10000 [==============================] - 2s 158us/step - loss: 0.2699 - acc: 0.7917 - val_loss: 0.2654 - val_acc: 0.7930\n",
      "Epoch 94/100\n",
      "10000/10000 [==============================] - 2s 159us/step - loss: 0.2713 - acc: 0.7918 - val_loss: 0.2644 - val_acc: 0.7927\n",
      "Epoch 95/100\n",
      "10000/10000 [==============================] - 2s 157us/step - loss: 0.2714 - acc: 0.7919 - val_loss: 0.2653 - val_acc: 0.7930\n",
      "Epoch 96/100\n",
      "10000/10000 [==============================] - 2s 162us/step - loss: 0.2710 - acc: 0.7919 - val_loss: 0.2665 - val_acc: 0.7920\n",
      "Epoch 97/100\n",
      "10000/10000 [==============================] - 2s 161us/step - loss: 0.2707 - acc: 0.7918 - val_loss: 0.2654 - val_acc: 0.7930\n",
      "Epoch 98/100\n",
      "10000/10000 [==============================] - 2s 165us/step - loss: 0.2708 - acc: 0.7917 - val_loss: 0.2648 - val_acc: 0.7924\n",
      "Epoch 99/100\n",
      "10000/10000 [==============================] - 2s 161us/step - loss: 0.2707 - acc: 0.7919 - val_loss: 0.2668 - val_acc: 0.7920\n",
      "Epoch 100/100\n",
      "10000/10000 [==============================] - 2s 166us/step - loss: 0.2702 - acc: 0.7919 - val_loss: 0.2652 - val_acc: 0.7918\n"
     ]
    }
   ],
   "source": [
    "history = ae.fit(x_tr, x_tr, \n",
    "                 batch_size=64,\n",
    "                 epochs=100, \n",
    "                 validation_data=(x_val, x_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(loss))\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Visualize the reconstructed test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_output = ae.predict(x_test).reshape((10000, 28, 28))\n",
    "\n",
    "ROW = 5\n",
    "COLUMN = 4\n",
    "\n",
    "x = ae_output\n",
    "fname = 'reconstruct_ae.pdf'\n",
    "\n",
    "fig, axes = plt.subplots(nrows=ROW, ncols=COLUMN, figsize=(8, 10))\n",
    "for ax, i in zip(axes.flat, numpy.arange(ROW*COLUMN)):\n",
    "    image = x[i].reshape(28, 28)\n",
    "    ax.imshow(image, cmap='gray')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(fname)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Evaluate the model on the test set\n",
    "\n",
    "Do NOT used the test set until now. Make sure that your model parameters and hyper-parameters are independent of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = ae.evaluate(x_test, x_test)\n",
    "print('loss = ' + str(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Visualize the low-dimensional features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the encoder network\n",
    "ae_encoder = models.Model(input_img, bottleneck)\n",
    "ae_encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract low-dimensional features from the test data\n",
    "encoded_test = ae_encoder.predict(x_test)\n",
    "print('Shape of encoded_test: ' + str(encoded_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = numpy.array(['r', 'g', 'b', 'm', 'c', 'k', 'y', 'purple', 'darkred', 'navy'])\n",
    "colors_test = colors[y_test]\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "plt.scatter(encoded_test[:, 0], encoded_test[:, 1], s=10, c=colors_test, edgecolors=colors_test)\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "fname = 'ae_code.pdf'\n",
    "plt.savefig(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remark:\n",
    "\n",
    "Judging from the visualization, the low-dim features seems not discriminative, as 2D features from different classes are mixed. Let quantatively find out whether they are discriminative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Are the learned low-dim features discriminative?\n",
    "\n",
    "To find the answer, lets train a classifier on the training set (the extracted 2-dim features) and evaluation on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the 2D features from the training, validation, and test samples\n",
    "f_tr = ae_encoder.predict(x_tr)\n",
    "f_val = ae_encoder.predict(x_val)\n",
    "f_te = ae_encoder.predict(x_test)\n",
    "\n",
    "print('Shape of f_tr: ' + str(f_tr.shape))\n",
    "print('Shape of f_te: ' + str(f_te.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input\n",
    "from keras import models\n",
    "\n",
    "input_feat = Input(shape=(2,))\n",
    "\n",
    "hidden1 = Dense(128, activation='relu')(input_feat)\n",
    "hidden2 = Dense(128, activation='relu')(hidden1)\n",
    "output = Dense(10, activation='softmax')(hidden2)\n",
    "\n",
    "classifier = models.Model(input_feat, output)\n",
    "\n",
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizers.RMSprop(lr=1E-4),\n",
    "                  metrics=['acc'])\n",
    "\n",
    "history = classifier.fit(f_tr, y_tr, \n",
    "                        batch_size=32, \n",
    "                        epochs=30, \n",
    "                        validation_data=(f_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Using the 2D features, the validation accuracy is 60~70%. Recall that using the original data, the accuracy is about 98%. Obviously, the 2D features are not very discriminative.\n",
    "\n",
    "We are going to build a supervised autoencode model for learning low-dimensional discriminative features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build a supervised autoencoder model\n",
    "\n",
    "\n",
    "**You are required to build and train a supervised autoencoder look like the following.** (Not necessary the same.) You are required to add other layers properly to alleviate overfitting.\n",
    "\n",
    "\n",
    "![Network Structure](https://github.com/wangshusen/CS583A-2019Spring/blob/master/homework/HM5/supervised_ae.png?raw=true \"NetworkStructure\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Build the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the supervised autoencoder network\n",
    "from keras.layers import Dense, Input\n",
    "from keras import models\n",
    "\n",
    "input_img = Input(shape=(784,), name='input_img')\n",
    "\n",
    "# encoder network\n",
    "encode1 = <add a dense layer taking input_img as input>\n",
    "<Add more layers...>\n",
    "# The width of the bottleneck layer must be exactly 2.\n",
    "bottleneck = <the output of encoder network>\n",
    "\n",
    "# decoder network\n",
    "decode1 = <add a dense layer taking bottleneck as input>\n",
    "<Add more layers...>\n",
    "decode4 = <the output of decoder network>\n",
    "\n",
    "# build a classifier upon the bottleneck layer\n",
    "classifier1 = <add a dense layer taking bottleneck as input>\n",
    "<Add more dense layers and regularizations...>\n",
    "classifier3 = <the output of classifier network>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect the input and the two outputs\n",
    "sae = models.Model(input_img, [decode4, classifier3])\n",
    "\n",
    "sae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the network structure to a PDF file\n",
    "\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot, plot_model\n",
    "\n",
    "SVG(model_to_dot(sae, show_shapes=False).create(prog='dot', format='svg'))\n",
    "\n",
    "plot_model(\n",
    "    model=sae, show_shapes=False,\n",
    "    to_file='supervised_ae.pdf'\n",
    ")\n",
    "\n",
    "# you can find the file \"supervised_ae.pdf\" in the current directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Train the new model and tune the hyper-parameters\n",
    "\n",
    "The new model has multiple output. Thus we specify **multiple** loss functions and their weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "sae.compile(loss=['mean_squared_error', 'categorical_crossentropy'],\n",
    "            loss_weights=[1, 0.5], # to be tuned\n",
    "            optimizer=optimizers.RMSprop(lr=1E-3))\n",
    "\n",
    "history = sae.fit(x_tr, [x_tr, y_tr], \n",
    "                  batch_size=32, \n",
    "                  epochs=100, \n",
    "                  validation_data=(x_val, [x_val, y_val]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(loss))\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "Do you think overfitting is happening? If yes, what can you do? Please make necessary changes to the supervised autoencoder network structure.\n",
    "\n",
    "**Failing to add proper regularization will lose 1~2 scores.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Visualize the reconstructed test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sae_output = sae.predict(x_test)[0].reshape((10000, 28, 28))\n",
    "\n",
    "ROW = 5\n",
    "COLUMN = 4\n",
    "\n",
    "x = sae_output\n",
    "fname = 'reconstruct_sae.pdf'\n",
    "\n",
    "fig, axes = plt.subplots(nrows=ROW, ncols=COLUMN, figsize=(8, 10))\n",
    "for ax, i in zip(axes.flat, numpy.arange(ROW*COLUMN)):\n",
    "    image = x[i].reshape(28, 28)\n",
    "    ax.imshow(image, cmap='gray')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(fname)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. Visualize the low-dimensional features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the encoder model\n",
    "sae_encoder = models.Model(input_img, bottleneck)\n",
    "sae_encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# extract test features\n",
    "encoded_test = sae_encoder.predict(x_test)\n",
    "print('Shape of encoded_test: ' + str(encoded_test.shape))\n",
    "\n",
    "colors = numpy.array(['r', 'g', 'b', 'm', 'c', 'k', 'y', 'purple', 'darkred', 'navy'])\n",
    "colors_test = colors[y_test]\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "plt.scatter(encoded_test[:, 0], encoded_test[:, 1], s=10, c=colors_test, edgecolors=colors_test)\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "fname = 'sae_code.pdf'\n",
    "plt.savefig(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5. Are the learned low-dim features discriminative?\n",
    "\n",
    "To find the answer, lets train a classifier on the training set (the extracted 2-dim features) and evaluation on the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract 2D features from the training, validation, and test samples\n",
    "f_tr = sae_encoder.predict(x_tr)\n",
    "f_val = sae_encoder.predict(x_val)\n",
    "f_te = sae_encoder.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a classifier which takes the 2D features as input\n",
    "from keras.layers import Dense, Input\n",
    "from keras import models\n",
    "\n",
    "input_feat = Input(shape=(2,))\n",
    "\n",
    "<build a classifier which takes input_feat as input>\n",
    "output = <output of the classifier network>\n",
    "\n",
    "classifier = models.Model(input_feat, output)\n",
    "\n",
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizers.RMSprop(lr=1E-4),\n",
    "                  metrics=['acc'])\n",
    "\n",
    "history = classifier.fit(f_tr, y_tr, \n",
    "                        batch_size=32, \n",
    "                        epochs=30, \n",
    "                        validation_data=(f_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remark:\n",
    "\n",
    "The validation accuracy must be above 90%. It means the low-dim features learned by the supervised autoencoder are very effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
