{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FGpab84RUE1z"
   },
   "source": [
    "# Home 3: Build a CNN for image recognition.\n",
    "\n",
    "### Name: [Gurpreet Singh]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Idt7_ZQ2UE11"
   },
   "source": [
    "## 0. You will do the following:\n",
    "\n",
    "1. Read, complete, and run my code.\n",
    "\n",
    "2. **Make substantial improvements** to maximize the accurcy.\n",
    "    \n",
    "3. Convert the .IPYNB file to .HTML file.\n",
    "\n",
    "    * The HTML file must contain the code and the output after execution.\n",
    "    \n",
    "4. Upload this .HTML file to your Github repo.\n",
    "\n",
    "4. Submit the link to this .HTML file to Canvas.\n",
    "\n",
    "    * Example: https://github.com/wangshusen/CS583A-2019Spring/blob/master/homework/HM3/cnn.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ungChk1LUE12"
   },
   "source": [
    "## 1. Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VuaKyJl3UE12"
   },
   "source": [
    "### 1.1. Load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "tT9tIf3RUE13",
    "outputId": "3df863ed-e211-4ad9-a7f0-622f7c856447"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of x_train: (50000, 32, 32, 3)\n",
      "shape of y_train: (50000, 1)\n",
      "shape of x_test: (10000, 32, 32, 3)\n",
      "shape of y_test: (10000, 1)\n",
      "number of classes: 10\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import cifar10\n",
    "import numpy\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "print('shape of x_train: ' + str(x_train.shape))\n",
    "print('shape of y_train: ' + str(y_train.shape))\n",
    "print('shape of x_test: ' + str(x_test.shape))\n",
    "print('shape of y_test: ' + str(y_test.shape))\n",
    "print('number of classes: ' + str(numpy.max(y_train) - numpy.min(y_train) + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fBa52YCiUE17"
   },
   "source": [
    "### 1.2. One-hot encode the labels\n",
    "\n",
    "In the input, a label is a scalar in $\\{0, 1, \\cdots , 9\\}$. One-hot encode transform such a scalar to a $10$-dim vector. E.g., a scalar ```y_train[j]=3``` is transformed to the vector ```y_train_vec[j]=[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]```.\n",
    "\n",
    "1. Define a function ```to_one_hot``` that transforms an $n\\times 1$ array to a $n\\times 10$ matrix.\n",
    "\n",
    "2. Apply the function to ```y_train``` and ```y_test```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "nn3v7gMbUE18",
    "outputId": "e73ab5c2-60e9-4e93-982d-00425de640c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y_train_vec: (50000, 10)\n",
      "Shape of y_test_vec: (10000, 10)\n",
      "[6]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "Num Classes: 10\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "\n",
    "y_train_vec = np_utils.to_categorical(y_train)\n",
    "y_test_vec = np_utils.to_categorical(y_test)\n",
    "\n",
    "num_classes = y_test_vec.shape[1]\n",
    "\n",
    "print('Shape of y_train_vec: ' + str(y_train_vec.shape))\n",
    "print('Shape of y_test_vec: ' + str(y_test_vec.shape))\n",
    "\n",
    "print(y_train[0])\n",
    "print(y_train_vec[0])\n",
    "print('Num Classes: ' + str(num_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qeuhqrGzUE1_"
   },
   "source": [
    "#### Remark: the outputs should be\n",
    "* Shape of y_train_vec: (50000, 10)\n",
    "* Shape of y_test_vec: (10000, 10)\n",
    "* [6]\n",
    "* [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X-kwHR7eUE2A"
   },
   "source": [
    "### 1.3. Randomly partition the training set to training and validation sets\n",
    "\n",
    "Randomly partition the 50K training samples to 2 sets:\n",
    "* a training set containing 40K samples\n",
    "* a validation set containing 10K samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "1gdgyWhTUE2A",
    "outputId": "c6cd6336-f6a0-47d2-c25a-3800122c0262"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_tr: (40000, 32, 32, 3)\n",
      "Shape of y_tr: (40000, 10)\n",
      "Shape of x_val: (10000, 32, 32, 3)\n",
      "Shape of y_val: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "rand_indices = numpy.random.permutation(50000)\n",
    "train_indices = rand_indices[0:40000]\n",
    "valid_indices = rand_indices[40000:50000]\n",
    "\n",
    "x_val = x_train[valid_indices, :]\n",
    "y_val = y_train_vec[valid_indices, :]\n",
    "\n",
    "x_tr = x_train[train_indices, :]\n",
    "y_tr = y_train_vec[train_indices, :]\n",
    "\n",
    "print('Shape of x_tr: ' + str(x_tr.shape))\n",
    "print('Shape of y_tr: ' + str(y_tr.shape))\n",
    "print('Shape of x_val: ' + str(x_val.shape))\n",
    "print('Shape of y_val: ' + str(y_val.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QWZ4KTTmUE2E"
   },
   "source": [
    "## 2. Build a CNN and tune its hyper-parameters\n",
    "\n",
    "1. Build a convolutional neural network model\n",
    "2. Use the validation data to tune the hyper-parameters (e.g., network structure, and optimization algorithm)\n",
    "    * Do NOT use test data for hyper-parameter tuning!!!\n",
    "3. Try to achieve a validation accuracy as high as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GKV_G6JTUE2F"
   },
   "source": [
    "### Remark: \n",
    "\n",
    "The following CNN is just an example. You are supposed to make **substantial improvements** such as:\n",
    "* Add more layers.\n",
    "* Use regularizations, e.g., dropout.\n",
    "* Use batch normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 748
    },
    "colab_type": "code",
    "id": "1TrmdpBeUE2G",
    "outputId": "c236dab6-411b-4cbf-a6cc-98307b114e0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               262272    \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 358,218\n",
      "Trainable params: 357,514\n",
      "Non-trainable params: 704\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, Activation\n",
    "from keras.models import Sequential\n",
    "\n",
    "IMAGE_SIZE = 32\n",
    "CHANNELS = 3\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# 32 filters AND kernel 3X3\n",
    "model.add(Conv2D(32, (3, 3), padding='same', input_shape=(IMAGE_SIZE, IMAGE_SIZE, CHANNELS)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(128))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CHafL1ejaP_M"
   },
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "learning_rate = 1E-4\n",
    "epochs = 100\n",
    "batch_size = 64\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=learning_rate),\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4cOHCimOagtz"
   },
   "outputs": [],
   "source": [
    "# data augmentation\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1)\n",
    "\n",
    "datagen.fit(x_tr)\n",
    "\n",
    "train_generator = datagen.flow(x_tr, y_tr, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3417
    },
    "colab_type": "code",
    "id": "C6d7vX-fwz1R",
    "outputId": "8a5b647a-f5e8-4ba9-be8c-890170c49d2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "625/625 [==============================] - 28s 45ms/step - loss: 1.8386 - acc: 0.3400 - val_loss: 1.4628 - val_acc: 0.4816\n",
      "Epoch 2/100\n",
      "625/625 [==============================] - 27s 43ms/step - loss: 1.5027 - acc: 0.4599 - val_loss: 1.3128 - val_acc: 0.5318\n",
      "Epoch 3/100\n",
      "625/625 [==============================] - 27s 43ms/step - loss: 1.3656 - acc: 0.5101 - val_loss: 1.2076 - val_acc: 0.5741\n",
      "Epoch 4/100\n",
      "625/625 [==============================] - 27s 43ms/step - loss: 1.2814 - acc: 0.5435 - val_loss: 1.1734 - val_acc: 0.5850\n",
      "Epoch 5/100\n",
      "625/625 [==============================] - 27s 43ms/step - loss: 1.2246 - acc: 0.5639 - val_loss: 1.1357 - val_acc: 0.5994\n",
      "Epoch 6/100\n",
      "625/625 [==============================] - 27s 43ms/step - loss: 1.1744 - acc: 0.5818 - val_loss: 1.0434 - val_acc: 0.6323\n",
      "Epoch 7/100\n",
      "625/625 [==============================] - 27s 43ms/step - loss: 1.1399 - acc: 0.5956 - val_loss: 1.0203 - val_acc: 0.6354\n",
      "Epoch 8/100\n",
      "625/625 [==============================] - 27s 43ms/step - loss: 1.1016 - acc: 0.6096 - val_loss: 0.9754 - val_acc: 0.6535\n",
      "Epoch 9/100\n",
      "625/625 [==============================] - 27s 43ms/step - loss: 1.0738 - acc: 0.6168 - val_loss: 1.0244 - val_acc: 0.6454\n",
      "Epoch 10/100\n",
      "625/625 [==============================] - 27s 43ms/step - loss: 1.0444 - acc: 0.6310 - val_loss: 1.0362 - val_acc: 0.6383\n",
      "Epoch 11/100\n",
      "625/625 [==============================] - 27s 43ms/step - loss: 1.0230 - acc: 0.6377 - val_loss: 1.0570 - val_acc: 0.6347\n",
      "Epoch 12/100\n",
      "625/625 [==============================] - 27s 43ms/step - loss: 1.0103 - acc: 0.6448 - val_loss: 1.0225 - val_acc: 0.6394\n",
      "Epoch 13/100\n",
      "625/625 [==============================] - 27s 44ms/step - loss: 0.9835 - acc: 0.6503 - val_loss: 0.8615 - val_acc: 0.6980\n",
      "Epoch 14/100\n",
      "625/625 [==============================] - 27s 43ms/step - loss: 0.9628 - acc: 0.6592 - val_loss: 0.9826 - val_acc: 0.6627\n",
      "Epoch 15/100\n",
      "625/625 [==============================] - 27s 43ms/step - loss: 0.9471 - acc: 0.6670 - val_loss: 0.9964 - val_acc: 0.6497\n",
      "Epoch 16/100\n",
      "625/625 [==============================] - 27s 43ms/step - loss: 0.9348 - acc: 0.6695 - val_loss: 0.9355 - val_acc: 0.6727\n",
      "Epoch 17/100\n",
      "625/625 [==============================] - 27s 44ms/step - loss: 0.9176 - acc: 0.6758 - val_loss: 0.8821 - val_acc: 0.6905\n",
      "Epoch 18/100\n",
      "625/625 [==============================] - 27s 43ms/step - loss: 0.9057 - acc: 0.6804 - val_loss: 0.8899 - val_acc: 0.6878\n",
      "Epoch 19/100\n",
      "625/625 [==============================] - 27s 43ms/step - loss: 0.8990 - acc: 0.6830 - val_loss: 1.0925 - val_acc: 0.6262\n",
      "Epoch 20/100\n",
      "625/625 [==============================] - 27s 43ms/step - loss: 0.8806 - acc: 0.6893 - val_loss: 0.9061 - val_acc: 0.6832\n",
      "Epoch 21/100\n",
      "625/625 [==============================] - 27s 43ms/step - loss: 0.8683 - acc: 0.6932 - val_loss: 0.8176 - val_acc: 0.7057\n",
      "Epoch 22/100\n",
      "625/625 [==============================] - 27s 43ms/step - loss: 0.8586 - acc: 0.6982 - val_loss: 0.9655 - val_acc: 0.6630\n",
      "Epoch 23/100\n",
      "625/625 [==============================] - 27s 43ms/step - loss: 0.8454 - acc: 0.7017 - val_loss: 0.7710 - val_acc: 0.7262\n",
      "Epoch 24/100\n",
      "625/625 [==============================] - 27s 43ms/step - loss: 0.8396 - acc: 0.7027 - val_loss: 0.8081 - val_acc: 0.7197\n",
      "Epoch 25/100\n",
      "625/625 [==============================] - 27s 43ms/step - loss: 0.8286 - acc: 0.7078 - val_loss: 0.8785 - val_acc: 0.6957\n",
      "Epoch 26/100\n",
      "625/625 [==============================] - 27s 43ms/step - loss: 0.8140 - acc: 0.7147 - val_loss: 0.7762 - val_acc: 0.7259\n",
      "Epoch 27/100\n",
      "625/625 [==============================] - 27s 43ms/step - loss: 0.8138 - acc: 0.7109 - val_loss: 0.8873 - val_acc: 0.6929\n",
      "Epoch 28/100\n",
      "625/625 [==============================] - 27s 43ms/step - loss: 0.8043 - acc: 0.7162 - val_loss: 0.8716 - val_acc: 0.6955\n",
      "Epoch 29/100\n",
      "625/625 [==============================] - 27s 43ms/step - loss: 0.7951 - acc: 0.7204 - val_loss: 0.8526 - val_acc: 0.7054\n",
      "Epoch 30/100\n",
      "625/625 [==============================] - 27s 43ms/step - loss: 0.7895 - acc: 0.7209 - val_loss: 0.8137 - val_acc: 0.7131\n",
      "Epoch 31/100\n",
      "625/625 [==============================] - 27s 43ms/step - loss: 0.7801 - acc: 0.7253 - val_loss: 0.7599 - val_acc: 0.7345\n",
      "Epoch 32/100\n",
      "625/625 [==============================] - 27s 43ms/step - loss: 0.7712 - acc: 0.7267 - val_loss: 0.7350 - val_acc: 0.7399\n",
      "Epoch 33/100\n",
      "625/625 [==============================] - 27s 43ms/step - loss: 0.7665 - acc: 0.7297 - val_loss: 0.8290 - val_acc: 0.7078\n",
      "Epoch 34/100\n",
      "625/625 [==============================] - 27s 43ms/step - loss: 0.7564 - acc: 0.7350 - val_loss: 0.7961 - val_acc: 0.7233\n",
      "Epoch 35/100\n",
      "625/625 [==============================] - 27s 43ms/step - loss: 0.7553 - acc: 0.7320 - val_loss: 0.8518 - val_acc: 0.7024\n",
      "Epoch 36/100\n",
      "625/625 [==============================] - 27s 42ms/step - loss: 0.7460 - acc: 0.7363 - val_loss: 0.7683 - val_acc: 0.7284\n",
      "Epoch 37/100\n",
      "625/625 [==============================] - 26s 42ms/step - loss: 0.7365 - acc: 0.7420 - val_loss: 0.7111 - val_acc: 0.7517\n",
      "Epoch 38/100\n",
      "625/625 [==============================] - 27s 43ms/step - loss: 0.7356 - acc: 0.7418 - val_loss: 0.6903 - val_acc: 0.7519\n",
      "Epoch 39/100\n",
      "625/625 [==============================] - 26s 42ms/step - loss: 0.7263 - acc: 0.7448 - val_loss: 0.7519 - val_acc: 0.7333\n",
      "Epoch 40/100\n",
      "625/625 [==============================] - 27s 43ms/step - loss: 0.7225 - acc: 0.7460 - val_loss: 0.6746 - val_acc: 0.7598\n",
      "Epoch 41/100\n",
      "625/625 [==============================] - 27s 43ms/step - loss: 0.7219 - acc: 0.7453 - val_loss: 0.8755 - val_acc: 0.7069\n",
      "Epoch 42/100\n",
      "625/625 [==============================] - 27s 43ms/step - loss: 0.7122 - acc: 0.7490 - val_loss: 0.7476 - val_acc: 0.7391\n",
      "Epoch 43/100\n",
      "625/625 [==============================] - 27s 43ms/step - loss: 0.7075 - acc: 0.7520 - val_loss: 0.8480 - val_acc: 0.7153\n",
      "Epoch 44/100\n",
      "625/625 [==============================] - 27s 43ms/step - loss: 0.7000 - acc: 0.7563 - val_loss: 0.7344 - val_acc: 0.7434\n",
      "Epoch 45/100\n",
      "625/625 [==============================] - 27s 43ms/step - loss: 0.6974 - acc: 0.7547 - val_loss: 0.8791 - val_acc: 0.7019\n",
      "Epoch 46/100\n",
      "625/625 [==============================] - 27s 43ms/step - loss: 0.6984 - acc: 0.7532 - val_loss: 0.6641 - val_acc: 0.7617\n",
      "Epoch 47/100\n",
      "625/625 [==============================] - 27s 43ms/step - loss: 0.6853 - acc: 0.7597 - val_loss: 0.6809 - val_acc: 0.7599\n",
      "Epoch 48/100\n",
      "625/625 [==============================] - 27s 43ms/step - loss: 0.6915 - acc: 0.7581 - val_loss: 0.6654 - val_acc: 0.7640\n",
      "Epoch 49/100\n",
      "625/625 [==============================] - 27s 43ms/step - loss: 0.6803 - acc: 0.7595 - val_loss: 0.7849 - val_acc: 0.7315\n",
      "Epoch 50/100\n",
      "625/625 [==============================] - 27s 43ms/step - loss: 0.6724 - acc: 0.7648 - val_loss: 0.6748 - val_acc: 0.7657\n",
      "Epoch 51/100\n",
      "625/625 [==============================] - 27s 43ms/step - loss: 0.6703 - acc: 0.7631 - val_loss: 0.6373 - val_acc: 0.7722\n",
      "Epoch 52/100\n",
      "625/625 [==============================] - 27s 43ms/step - loss: 0.6631 - acc: 0.7668 - val_loss: 0.7101 - val_acc: 0.7483\n",
      "Epoch 53/100\n",
      "625/625 [==============================] - 27s 43ms/step - loss: 0.6615 - acc: 0.7677 - val_loss: 0.7439 - val_acc: 0.7449\n",
      "Epoch 54/100\n",
      "625/625 [==============================] - 27s 43ms/step - loss: 0.6557 - acc: 0.7702 - val_loss: 0.8066 - val_acc: 0.7350\n",
      "Epoch 55/100\n",
      "625/625 [==============================] - 27s 43ms/step - loss: 0.6500 - acc: 0.7722 - val_loss: 0.7448 - val_acc: 0.7451\n",
      "Epoch 56/100\n",
      "625/625 [==============================] - 27s 43ms/step - loss: 0.6466 - acc: 0.7702 - val_loss: 0.7997 - val_acc: 0.7323\n",
      "Epoch 57/100\n",
      "625/625 [==============================] - 27s 43ms/step - loss: 0.6456 - acc: 0.7723 - val_loss: 0.6813 - val_acc: 0.7604\n",
      "Epoch 58/100\n",
      "625/625 [==============================] - 27s 43ms/step - loss: 0.6437 - acc: 0.7740 - val_loss: 0.7864 - val_acc: 0.7321\n",
      "Epoch 59/100\n",
      "625/625 [==============================] - 27s 43ms/step - loss: 0.6419 - acc: 0.7747 - val_loss: 0.6531 - val_acc: 0.7672\n",
      "Epoch 60/100\n",
      "625/625 [==============================] - 27s 43ms/step - loss: 0.6327 - acc: 0.7780 - val_loss: 0.7231 - val_acc: 0.7542\n",
      "Epoch 61/100\n",
      "625/625 [==============================] - 27s 43ms/step - loss: 0.6300 - acc: 0.7772 - val_loss: 0.7085 - val_acc: 0.7598\n",
      "Epoch 62/100\n",
      "625/625 [==============================] - 27s 43ms/step - loss: 0.6225 - acc: 0.7817 - val_loss: 0.6642 - val_acc: 0.7646\n",
      "Epoch 63/100\n",
      "625/625 [==============================] - 27s 43ms/step - loss: 0.6222 - acc: 0.7819 - val_loss: 0.6025 - val_acc: 0.7892\n",
      "Epoch 64/100\n",
      "625/625 [==============================] - 27s 43ms/step - loss: 0.6197 - acc: 0.7820 - val_loss: 0.7295 - val_acc: 0.7460\n",
      "Epoch 65/100\n",
      "625/625 [==============================] - 27s 43ms/step - loss: 0.6154 - acc: 0.7816 - val_loss: 0.6656 - val_acc: 0.7641\n",
      "Epoch 66/100\n",
      "625/625 [==============================] - 27s 43ms/step - loss: 0.6117 - acc: 0.7840 - val_loss: 0.6728 - val_acc: 0.7679\n",
      "Epoch 67/100\n",
      "625/625 [==============================] - 27s 43ms/step - loss: 0.6114 - acc: 0.7866 - val_loss: 0.6929 - val_acc: 0.7562\n",
      "Epoch 68/100\n",
      "625/625 [==============================] - 27s 43ms/step - loss: 0.6084 - acc: 0.7856 - val_loss: 0.6658 - val_acc: 0.7691\n",
      "Epoch 69/100\n",
      "625/625 [==============================] - 27s 43ms/step - loss: 0.6025 - acc: 0.7914 - val_loss: 0.5768 - val_acc: 0.7974\n",
      "Epoch 70/100\n",
      "625/625 [==============================] - 27s 43ms/step - loss: 0.5952 - acc: 0.7907 - val_loss: 0.6565 - val_acc: 0.7731\n",
      "Epoch 71/100\n",
      "625/625 [==============================] - 27s 43ms/step - loss: 0.5994 - acc: 0.7899 - val_loss: 0.6016 - val_acc: 0.7918\n",
      "Epoch 72/100\n",
      "625/625 [==============================] - 27s 42ms/step - loss: 0.5986 - acc: 0.7887 - val_loss: 0.6634 - val_acc: 0.7724\n",
      "Epoch 73/100\n",
      "625/625 [==============================] - 27s 43ms/step - loss: 0.5904 - acc: 0.7942 - val_loss: 0.7660 - val_acc: 0.7378\n",
      "Epoch 74/100\n",
      "625/625 [==============================] - 27s 43ms/step - loss: 0.5861 - acc: 0.7915 - val_loss: 0.6205 - val_acc: 0.7837\n",
      "Epoch 75/100\n",
      "625/625 [==============================] - 27s 43ms/step - loss: 0.5885 - acc: 0.7924 - val_loss: 0.6189 - val_acc: 0.7842\n",
      "Epoch 76/100\n",
      "625/625 [==============================] - 27s 43ms/step - loss: 0.5859 - acc: 0.7947 - val_loss: 0.6811 - val_acc: 0.7693\n",
      "Epoch 77/100\n",
      "625/625 [==============================] - 27s 43ms/step - loss: 0.5825 - acc: 0.7953 - val_loss: 0.6036 - val_acc: 0.7879\n",
      "Epoch 78/100\n",
      "625/625 [==============================] - 27s 43ms/step - loss: 0.5786 - acc: 0.7966 - val_loss: 0.6553 - val_acc: 0.7685\n",
      "Epoch 79/100\n",
      "625/625 [==============================] - 27s 43ms/step - loss: 0.5757 - acc: 0.7981 - val_loss: 0.6700 - val_acc: 0.7678\n",
      "Epoch 80/100\n",
      "625/625 [==============================] - 26s 42ms/step - loss: 0.5720 - acc: 0.8004 - val_loss: 0.5855 - val_acc: 0.7925\n",
      "Epoch 81/100\n",
      "625/625 [==============================] - 26s 42ms/step - loss: 0.5738 - acc: 0.7984 - val_loss: 0.7204 - val_acc: 0.7584\n",
      "Epoch 82/100\n",
      "625/625 [==============================] - 26s 42ms/step - loss: 0.5712 - acc: 0.8000 - val_loss: 0.6035 - val_acc: 0.7886\n",
      "Epoch 83/100\n",
      "625/625 [==============================] - 27s 42ms/step - loss: 0.5642 - acc: 0.8033 - val_loss: 0.6276 - val_acc: 0.7824\n",
      "Epoch 84/100\n",
      "625/625 [==============================] - 26s 42ms/step - loss: 0.5645 - acc: 0.8018 - val_loss: 0.6483 - val_acc: 0.7718\n",
      "Epoch 85/100\n",
      "625/625 [==============================] - 26s 42ms/step - loss: 0.5615 - acc: 0.8017 - val_loss: 0.6001 - val_acc: 0.7897\n",
      "Epoch 86/100\n",
      "625/625 [==============================] - 27s 43ms/step - loss: 0.5598 - acc: 0.8024 - val_loss: 0.6231 - val_acc: 0.7852\n",
      "Epoch 87/100\n",
      "625/625 [==============================] - 26s 42ms/step - loss: 0.5560 - acc: 0.8056 - val_loss: 0.6229 - val_acc: 0.7863\n",
      "Epoch 88/100\n",
      "625/625 [==============================] - 26s 42ms/step - loss: 0.5538 - acc: 0.8051 - val_loss: 0.6529 - val_acc: 0.7801\n",
      "Epoch 89/100\n",
      "625/625 [==============================] - 26s 42ms/step - loss: 0.5494 - acc: 0.8086 - val_loss: 0.6136 - val_acc: 0.7902\n",
      "Epoch 90/100\n",
      "625/625 [==============================] - 26s 42ms/step - loss: 0.5505 - acc: 0.8061 - val_loss: 0.7785 - val_acc: 0.7448\n",
      "Epoch 91/100\n",
      "625/625 [==============================] - 27s 42ms/step - loss: 0.5500 - acc: 0.8055 - val_loss: 0.5899 - val_acc: 0.7929\n",
      "Epoch 92/100\n",
      "625/625 [==============================] - 27s 43ms/step - loss: 0.5464 - acc: 0.8098 - val_loss: 0.6462 - val_acc: 0.7807\n",
      "Epoch 93/100\n",
      "625/625 [==============================] - 27s 42ms/step - loss: 0.5435 - acc: 0.8085 - val_loss: 0.6440 - val_acc: 0.7792\n",
      "Epoch 94/100\n",
      "625/625 [==============================] - 26s 42ms/step - loss: 0.5457 - acc: 0.8083 - val_loss: 0.5998 - val_acc: 0.7941\n",
      "Epoch 95/100\n",
      "625/625 [==============================] - 26s 42ms/step - loss: 0.5379 - acc: 0.8100 - val_loss: 0.8367 - val_acc: 0.7338\n",
      "Epoch 96/100\n",
      "625/625 [==============================] - 26s 42ms/step - loss: 0.5333 - acc: 0.8129 - val_loss: 0.6162 - val_acc: 0.7900\n",
      "Epoch 97/100\n",
      "625/625 [==============================] - 26s 42ms/step - loss: 0.5428 - acc: 0.8084 - val_loss: 0.7094 - val_acc: 0.7558\n",
      "Epoch 98/100\n",
      "625/625 [==============================] - 27s 43ms/step - loss: 0.5384 - acc: 0.8112 - val_loss: 0.6792 - val_acc: 0.7772\n",
      "Epoch 99/100\n",
      "625/625 [==============================] - 26s 42ms/step - loss: 0.5302 - acc: 0.8155 - val_loss: 0.6484 - val_acc: 0.7848\n",
      "Epoch 100/100\n",
      "625/625 [==============================] - 26s 42ms/step - loss: 0.5362 - acc: 0.8121 - val_loss: 0.7174 - val_acc: 0.7638\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(train_generator,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    steps_per_epoch=len(x_tr)/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "colab_type": "code",
    "id": "5aedPzwAt5ED",
    "outputId": "44805328-82fe-4735-b33a-5bde313b4046"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEGCAYAAABsLkJ6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEroLzacAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ueNvVH0jUE2L"
   },
   "outputs": [],
   "source": [
    "# history = model.fit(x_tr, y_tr, batch_size=128, epochs=25, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vNngkLK_UE2Q"
   },
   "source": [
    "## 3. Train (again) and evaluate the model\n",
    "\n",
    "- To this end, you have found the \"best\" hyper-parameters. \n",
    "- Now, fix the hyper-parameters and train the network on the entire training set (all the 50K training samples)\n",
    "- Evaluate your model on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PuBT_p_vUE2R"
   },
   "source": [
    "### 3.1. Train the model on the entire training set\n",
    "\n",
    "Why? Previously, you used 40K samples for training; you wasted 10K samples for the sake of hyper-parameter tuning. Now you already know the hyper-parameters, so why not using all the 50K samples for training?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 4148
    },
    "colab_type": "code",
    "id": "OZFklNaMUE2S",
    "outputId": "f5667f98-3613-4ad1-db9a-b53a5f9429ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_10 (Conv2D)           (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 128)               262272    \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 358,218\n",
      "Trainable params: 357,514\n",
      "Non-trainable params: 704\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "782/781 [==============================] - 34s 43ms/step - loss: 1.7710 - acc: 0.3633\n",
      "Epoch 2/100\n",
      "782/781 [==============================] - 32s 41ms/step - loss: 1.4472 - acc: 0.4794\n",
      "Epoch 3/100\n",
      "782/781 [==============================] - 32s 41ms/step - loss: 1.3184 - acc: 0.5268\n",
      "Epoch 4/100\n",
      "782/781 [==============================] - 32s 41ms/step - loss: 1.2336 - acc: 0.5586\n",
      "Epoch 5/100\n",
      "782/781 [==============================] - 32s 41ms/step - loss: 1.1668 - acc: 0.5834\n",
      "Epoch 6/100\n",
      "782/781 [==============================] - 32s 41ms/step - loss: 1.1123 - acc: 0.6067\n",
      "Epoch 7/100\n",
      "782/781 [==============================] - 32s 41ms/step - loss: 1.0765 - acc: 0.6203\n",
      "Epoch 8/100\n",
      "782/781 [==============================] - 32s 41ms/step - loss: 1.0452 - acc: 0.6315\n",
      "Epoch 9/100\n",
      "782/781 [==============================] - 33s 42ms/step - loss: 1.0158 - acc: 0.6402\n",
      "Epoch 10/100\n",
      "782/781 [==============================] - 32s 42ms/step - loss: 0.9913 - acc: 0.6485\n",
      "Epoch 11/100\n",
      "782/781 [==============================] - 32s 41ms/step - loss: 0.9677 - acc: 0.6588\n",
      "Epoch 12/100\n",
      "782/781 [==============================] - 32s 41ms/step - loss: 0.9506 - acc: 0.6639\n",
      "Epoch 13/100\n",
      "782/781 [==============================] - 32s 41ms/step - loss: 0.9295 - acc: 0.6713\n",
      "Epoch 14/100\n",
      "782/781 [==============================] - 32s 41ms/step - loss: 0.9084 - acc: 0.6774\n",
      "Epoch 15/100\n",
      "782/781 [==============================] - 32s 42ms/step - loss: 0.8997 - acc: 0.6834\n",
      "Epoch 16/100\n",
      "782/781 [==============================] - 32s 41ms/step - loss: 0.8865 - acc: 0.6880\n",
      "Epoch 17/100\n",
      "782/781 [==============================] - 32s 41ms/step - loss: 0.8705 - acc: 0.6919\n",
      "Epoch 18/100\n",
      "782/781 [==============================] - 32s 41ms/step - loss: 0.8572 - acc: 0.6977\n",
      "Epoch 19/100\n",
      "782/781 [==============================] - 32s 41ms/step - loss: 0.8467 - acc: 0.7022\n",
      "Epoch 20/100\n",
      "782/781 [==============================] - 32s 41ms/step - loss: 0.8394 - acc: 0.7045\n",
      "Epoch 21/100\n",
      "782/781 [==============================] - 32s 41ms/step - loss: 0.8299 - acc: 0.7069\n",
      "Epoch 22/100\n",
      "782/781 [==============================] - 32s 41ms/step - loss: 0.8150 - acc: 0.7157\n",
      "Epoch 23/100\n",
      "782/781 [==============================] - 32s 41ms/step - loss: 0.8098 - acc: 0.7163\n",
      "Epoch 24/100\n",
      "782/781 [==============================] - 32s 41ms/step - loss: 0.7977 - acc: 0.7175\n",
      "Epoch 25/100\n",
      "782/781 [==============================] - 32s 41ms/step - loss: 0.7874 - acc: 0.7224\n",
      "Epoch 26/100\n",
      "782/781 [==============================] - 32s 40ms/step - loss: 0.7811 - acc: 0.7243\n",
      "Epoch 27/100\n",
      "782/781 [==============================] - 32s 41ms/step - loss: 0.7741 - acc: 0.7261\n",
      "Epoch 28/100\n",
      "782/781 [==============================] - 32s 41ms/step - loss: 0.7652 - acc: 0.7320\n",
      "Epoch 29/100\n",
      "782/781 [==============================] - 32s 41ms/step - loss: 0.7589 - acc: 0.7323\n",
      "Epoch 30/100\n",
      "782/781 [==============================] - 32s 40ms/step - loss: 0.7527 - acc: 0.7346\n",
      "Epoch 31/100\n",
      "782/781 [==============================] - 32s 40ms/step - loss: 0.7435 - acc: 0.7380\n",
      "Epoch 32/100\n",
      "782/781 [==============================] - 32s 40ms/step - loss: 0.7360 - acc: 0.7400\n",
      "Epoch 33/100\n",
      "782/781 [==============================] - 32s 40ms/step - loss: 0.7301 - acc: 0.7450\n",
      "Epoch 34/100\n",
      "782/781 [==============================] - 32s 41ms/step - loss: 0.7263 - acc: 0.7434\n",
      "Epoch 35/100\n",
      "782/781 [==============================] - 32s 41ms/step - loss: 0.7169 - acc: 0.7482\n",
      "Epoch 36/100\n",
      "782/781 [==============================] - 32s 41ms/step - loss: 0.7126 - acc: 0.7487\n",
      "Epoch 37/100\n",
      "782/781 [==============================] - 32s 41ms/step - loss: 0.7035 - acc: 0.7512\n",
      "Epoch 38/100\n",
      "782/781 [==============================] - 32s 41ms/step - loss: 0.7022 - acc: 0.7515\n",
      "Epoch 39/100\n",
      "782/781 [==============================] - 32s 41ms/step - loss: 0.6953 - acc: 0.7563\n",
      "Epoch 40/100\n",
      "782/781 [==============================] - 32s 41ms/step - loss: 0.6873 - acc: 0.7563\n",
      "Epoch 41/100\n",
      "782/781 [==============================] - 32s 41ms/step - loss: 0.6872 - acc: 0.7582\n",
      "Epoch 42/100\n",
      "782/781 [==============================] - 32s 41ms/step - loss: 0.6842 - acc: 0.7600\n",
      "Epoch 43/100\n",
      "782/781 [==============================] - 32s 41ms/step - loss: 0.6786 - acc: 0.7624\n",
      "Epoch 44/100\n",
      "782/781 [==============================] - 32s 41ms/step - loss: 0.6710 - acc: 0.7633\n",
      "Epoch 45/100\n",
      "782/781 [==============================] - 32s 41ms/step - loss: 0.6611 - acc: 0.7675\n",
      "Epoch 46/100\n",
      "782/781 [==============================] - 32s 41ms/step - loss: 0.6580 - acc: 0.7702\n",
      "Epoch 47/100\n",
      "782/781 [==============================] - 32s 41ms/step - loss: 0.6541 - acc: 0.7705\n",
      "Epoch 48/100\n",
      "782/781 [==============================] - 32s 41ms/step - loss: 0.6515 - acc: 0.7696\n",
      "Epoch 49/100\n",
      "782/781 [==============================] - 32s 41ms/step - loss: 0.6465 - acc: 0.7714\n",
      "Epoch 50/100\n",
      "782/781 [==============================] - 32s 41ms/step - loss: 0.6419 - acc: 0.7746\n",
      "Epoch 51/100\n",
      "782/781 [==============================] - 32s 41ms/step - loss: 0.6391 - acc: 0.7742\n",
      "Epoch 52/100\n",
      "782/781 [==============================] - 32s 41ms/step - loss: 0.6340 - acc: 0.7781\n",
      "Epoch 53/100\n",
      "782/781 [==============================] - 32s 41ms/step - loss: 0.6352 - acc: 0.7775\n",
      "Epoch 54/100\n",
      "782/781 [==============================] - 32s 41ms/step - loss: 0.6299 - acc: 0.7798\n",
      "Epoch 55/100\n",
      "782/781 [==============================] - 32s 41ms/step - loss: 0.6207 - acc: 0.7815\n",
      "Epoch 56/100\n",
      "782/781 [==============================] - 32s 41ms/step - loss: 0.6227 - acc: 0.7815\n",
      "Epoch 57/100\n",
      "782/781 [==============================] - 32s 41ms/step - loss: 0.6189 - acc: 0.7842\n",
      "Epoch 58/100\n",
      "782/781 [==============================] - 32s 41ms/step - loss: 0.6106 - acc: 0.7864\n",
      "Epoch 59/100\n",
      "782/781 [==============================] - 32s 41ms/step - loss: 0.6093 - acc: 0.7872\n",
      "Epoch 60/100\n",
      "782/781 [==============================] - 32s 41ms/step - loss: 0.6088 - acc: 0.7874\n",
      "Epoch 61/100\n",
      "782/781 [==============================] - 32s 41ms/step - loss: 0.6031 - acc: 0.7900\n",
      "Epoch 62/100\n",
      "782/781 [==============================] - 32s 41ms/step - loss: 0.6011 - acc: 0.7899\n",
      "Epoch 63/100\n",
      "782/781 [==============================] - 32s 41ms/step - loss: 0.5978 - acc: 0.7897\n",
      "Epoch 64/100\n",
      "782/781 [==============================] - 32s 41ms/step - loss: 0.5932 - acc: 0.7934\n",
      "Epoch 65/100\n",
      "782/781 [==============================] - 32s 41ms/step - loss: 0.5915 - acc: 0.7925\n",
      "Epoch 66/100\n",
      "782/781 [==============================] - 32s 41ms/step - loss: 0.5868 - acc: 0.7946\n",
      "Epoch 67/100\n",
      "782/781 [==============================] - 32s 41ms/step - loss: 0.5886 - acc: 0.7950\n",
      "Epoch 68/100\n",
      "782/781 [==============================] - 32s 41ms/step - loss: 0.5822 - acc: 0.7956\n",
      "Epoch 69/100\n",
      "782/781 [==============================] - 32s 41ms/step - loss: 0.5789 - acc: 0.7976\n",
      "Epoch 70/100\n",
      "782/781 [==============================] - 32s 41ms/step - loss: 0.5806 - acc: 0.7963\n",
      "Epoch 71/100\n",
      "782/781 [==============================] - 33s 42ms/step - loss: 0.5768 - acc: 0.7986\n",
      "Epoch 72/100\n",
      "782/781 [==============================] - 32s 41ms/step - loss: 0.5730 - acc: 0.8000\n",
      "Epoch 73/100\n",
      "782/781 [==============================] - 32s 41ms/step - loss: 0.5700 - acc: 0.7982\n",
      "Epoch 74/100\n",
      "782/781 [==============================] - 32s 41ms/step - loss: 0.5678 - acc: 0.8030\n",
      "Epoch 75/100\n",
      "782/781 [==============================] - 32s 40ms/step - loss: 0.5679 - acc: 0.8016\n",
      "Epoch 76/100\n",
      "782/781 [==============================] - 32s 41ms/step - loss: 0.5671 - acc: 0.8013\n",
      "Epoch 77/100\n",
      "782/781 [==============================] - 32s 41ms/step - loss: 0.5602 - acc: 0.8045\n",
      "Epoch 78/100\n",
      "782/781 [==============================] - 32s 40ms/step - loss: 0.5559 - acc: 0.8058\n",
      "Epoch 79/100\n",
      "782/781 [==============================] - 32s 40ms/step - loss: 0.5507 - acc: 0.8067\n",
      "Epoch 80/100\n",
      "782/781 [==============================] - 32s 41ms/step - loss: 0.5493 - acc: 0.8077\n",
      "Epoch 81/100\n",
      "782/781 [==============================] - 32s 41ms/step - loss: 0.5501 - acc: 0.8076\n",
      "Epoch 82/100\n",
      "782/781 [==============================] - 32s 41ms/step - loss: 0.5435 - acc: 0.8093\n",
      "Epoch 83/100\n",
      "782/781 [==============================] - 32s 41ms/step - loss: 0.5419 - acc: 0.8097\n",
      "Epoch 84/100\n",
      "782/781 [==============================] - 32s 41ms/step - loss: 0.5380 - acc: 0.8097\n",
      "Epoch 85/100\n",
      "782/781 [==============================] - 32s 41ms/step - loss: 0.5449 - acc: 0.8086\n",
      "Epoch 86/100\n",
      "782/781 [==============================] - 32s 41ms/step - loss: 0.5414 - acc: 0.8095\n",
      "Epoch 87/100\n",
      "782/781 [==============================] - 32s 41ms/step - loss: 0.5371 - acc: 0.8129\n",
      "Epoch 88/100\n",
      "782/781 [==============================] - 32s 41ms/step - loss: 0.5315 - acc: 0.8146\n",
      "Epoch 89/100\n",
      "782/781 [==============================] - 32s 41ms/step - loss: 0.5311 - acc: 0.8129\n",
      "Epoch 90/100\n",
      "782/781 [==============================] - 32s 41ms/step - loss: 0.5314 - acc: 0.8146\n",
      "Epoch 91/100\n",
      "782/781 [==============================] - 32s 41ms/step - loss: 0.5295 - acc: 0.8146\n",
      "Epoch 92/100\n",
      "782/781 [==============================] - 32s 41ms/step - loss: 0.5204 - acc: 0.8172\n",
      "Epoch 93/100\n",
      "782/781 [==============================] - 32s 40ms/step - loss: 0.5227 - acc: 0.8167\n",
      "Epoch 94/100\n",
      "782/781 [==============================] - 32s 41ms/step - loss: 0.5201 - acc: 0.8176\n",
      "Epoch 95/100\n",
      "782/781 [==============================] - 32s 41ms/step - loss: 0.5250 - acc: 0.8165\n",
      "Epoch 96/100\n",
      "782/781 [==============================] - 32s 41ms/step - loss: 0.5180 - acc: 0.8201\n",
      "Epoch 97/100\n",
      "782/781 [==============================] - 32s 41ms/step - loss: 0.5201 - acc: 0.8179\n",
      "Epoch 98/100\n",
      "782/781 [==============================] - 32s 41ms/step - loss: 0.5222 - acc: 0.8179\n",
      "Epoch 99/100\n",
      "782/781 [==============================] - 32s 41ms/step - loss: 0.5096 - acc: 0.8218\n",
      "Epoch 100/100\n",
      "782/781 [==============================] - 32s 41ms/step - loss: 0.5101 - acc: 0.8201\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, Activation\n",
    "from keras.models import Sequential\n",
    "\n",
    "IMAGE_SIZE = 32\n",
    "CHANNELS = 3\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# 32 filters AND kernel 3X3\n",
    "model.add(Conv2D(32, (3, 3), padding='same', input_shape=(IMAGE_SIZE, IMAGE_SIZE, CHANNELS)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(128))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "learning_rate = 1E-4\n",
    "epochs = 100\n",
    "batch_size = 64\n",
    "\n",
    "datagen.fit(x_train)\n",
    "\n",
    "train_generator = datagen.flow(x_train, y_train_vec, batch_size=batch_size)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=learning_rate),\n",
    "              metrics=['acc'])\n",
    "\n",
    "history = model.fit_generator(train_generator,\n",
    "                    epochs=epochs,\n",
    "                    steps_per_epoch=len(x_train)/batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5vloUlQWUE2W"
   },
   "source": [
    "### 3.2. Evaluate the model on the test set\n",
    "\n",
    "Do NOT used the test set until now. Make sure that your model parameters and hyper-parameters are independent of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "O-JKuROSUE2W",
    "outputId": "4d558140-ab6b-4410-8f08-16a4e4d37344"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 2s 191us/step\n",
      "loss = 0.5533049704551697\n",
      "accuracy = 0.8152\n"
     ]
    }
   ],
   "source": [
    "loss_and_acc = model.evaluate(x_test, y_test_vec)\n",
    "print('loss = ' + str(loss_and_acc[0]))\n",
    "print('accuracy = ' + str(loss_and_acc[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yWX58vPkUE2Y"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "HW3.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
